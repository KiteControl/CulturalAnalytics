{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0c1b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import osmnx as ox\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class BoundingBox:\n",
    "    def __init__(self, minLon, maxLon, minLat, maxLat):\n",
    "        self.minLon = minLon\n",
    "        self.maxLon = maxLon\n",
    "        self.minLat = minLat\n",
    "        self.maxLat = maxLat\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"BoundingBox(lon: {self.minLon:.6f} to {self.maxLon:.6f}, lat: {self.minLat:.6f} to {self.maxLat:.6f})\"\n",
    "\n",
    "class CityRasterizer:\n",
    "    def __init__(self, place_name=None, country=None, continent=None, bounding_box=None, grid_width=1000, grid_height=1000, gdp=None, population=None):\n",
    "        \"\"\"\n",
    "        Initialize the city rasterizer with either a place name or bounding box.\n",
    "        \n",
    "        Args:\n",
    "            place_name (str): Name of the place to rasterize (e.g., \"Munich, Germany\")\n",
    "            country (str): Name of the country\n",
    "            continent (str): Name of the continent\n",
    "            bounding_box (BoundingBox): Geographic bounding box (alternative to place_name)\n",
    "            grid_width (int): Number of cells in x-direction\n",
    "            grid_height (int): Number of cells in y-direction\n",
    "            gdp (float): Nominal GDP in USD billions\n",
    "            population (int): Population of the city\n",
    "        \"\"\"\n",
    "        if place_name is None and bounding_box is None:\n",
    "            raise ValueError(\"Either place_name or bounding_box must be provided\")\n",
    "        \n",
    "        self.place_name = place_name\n",
    "        self.country = country\n",
    "        self.continent = continent\n",
    "        self.bounding_box = bounding_box\n",
    "        self.grid_width = grid_width\n",
    "        self.grid_height = grid_height\n",
    "        self.grid = None\n",
    "        self.buildings = None\n",
    "        self.gdp = gdp\n",
    "        self.population = population\n",
    "        self.transformer = None\n",
    "        self.transform = None\n",
    "        self.cell_width = None\n",
    "        self.cell_height = None\n",
    "        self.x_min = None\n",
    "        self.y_min = None\n",
    "        self.x_max = None\n",
    "        self.y_max = None\n",
    "        \n",
    "        if self.place_name and self.bounding_box is None:\n",
    "            self._get_place_bounds()\n",
    "        \n",
    "        if self.bounding_box:\n",
    "            self._initialize_grid()\n",
    "            self._process_buildings()\n",
    "\n",
    "    def _get_place_bounds(self):\n",
    "        \"\"\"Get bounding box from place name\"\"\"\n",
    "        print(f\"Getting bounds for {self.place_name}...\")\n",
    "        try:\n",
    "            place_gdf = ox.geocode_to_gdf(self.place_name)\n",
    "            bounds = place_gdf.total_bounds\n",
    "            self.bounding_box = BoundingBox(\n",
    "                minLon=bounds[0], maxLon=bounds[2],\n",
    "                minLat=bounds[1], maxLat=bounds[3]\n",
    "            )\n",
    "            print(f\"Bounding box: {self.bounding_box}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {self.place_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_grid(self):\n",
    "        \"\"\"Initialize the grid with coordinate transformations\"\"\"\n",
    "        self.transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n",
    "        \n",
    "        # Transform bounds to Web Mercator\n",
    "        self.x_min, self.y_min = self.transformer.transform(\n",
    "            self.bounding_box.minLon, self.bounding_box.minLat\n",
    "        )\n",
    "        self.x_max, self.y_max = self.transformer.transform(\n",
    "            self.bounding_box.maxLon, self.bounding_box.maxLat\n",
    "        )\n",
    "        \n",
    "        self.cell_width = (self.x_max - self.x_min) / self.grid_width\n",
    "        self.cell_height = (self.y_max - self.y_min) / self.grid_height\n",
    "        \n",
    "        self.transform = from_origin(\n",
    "            west=self.x_min, \n",
    "            north=self.y_max,\n",
    "            xsize=self.cell_width, \n",
    "            ysize=self.cell_height\n",
    "        )\n",
    "        \n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "        \n",
    "        print(f\"Grid initialized: {self.grid_width}x{self.grid_height}\")\n",
    "        print(f\"Cell size: {self.cell_width:.2f}m x {self.cell_height:.2f}m\")\n",
    "\n",
    "    def _fetch_osm_buildings(self):\n",
    "        \"\"\"Fetch building data from OSM with error handling\"\"\"\n",
    "        print(f\"Fetching building data for {self.place_name or 'custom bounding box'}...\")\n",
    "        \n",
    "        try:\n",
    "            if self.place_name:\n",
    "                buildings = ox.features_from_place(self.place_name, tags={'building': True})\n",
    "            else:\n",
    "                buildings = ox.features_from_bbox(\n",
    "                    self.bounding_box.minLat, self.bounding_box.maxLat,\n",
    "                    self.bounding_box.minLon, self.bounding_box.maxLon, \n",
    "                    tags={'building': True}\n",
    "                )\n",
    "            \n",
    "            if buildings.empty or not isinstance(buildings, gpd.GeoDataFrame):\n",
    "                print(f\"No valid building data returned for {self.place_name or 'custom bounding box'}.\")\n",
    "                return gpd.GeoDataFrame()\n",
    "            \n",
    "            if 'geometry' not in buildings.columns or buildings.geometry.isna().all():\n",
    "                print(f\"Invalid or missing geometry data for {self.place_name or 'custom bounding box'}.\")\n",
    "                return gpd.GeoDataFrame()\n",
    "            \n",
    "            return buildings\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching buildings for {self.place_name or 'custom bounding box'}: {e}\")\n",
    "            return gpd.GeoDataFrame()\n",
    "\n",
    "    def _process_buildings(self, min_buildings=1000):\n",
    "        \"\"\"Process building data, rasterize if enough buildings exist\n",
    "        Args:\n",
    "            min_buildings (int): Minimum number of buildings required to process\n",
    "        \"\"\"\n",
    "        self.buildings = self._fetch_osm_buildings()\n",
    "        \n",
    "        if self.buildings.empty:\n",
    "            print(f\"No buildings found for {self.place_name or 'custom bounding box'}. Skipping.\")\n",
    "            self._cleanup()\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            building_count = len(self.buildings)\n",
    "            print(f\"Found {building_count} buildings in OSM data\")\n",
    "            \n",
    "            if building_count < min_buildings:\n",
    "                print(f\"Skipping {self.place_name} - only {building_count} buildings found (needs {min_buildings}+)\")\n",
    "                self._cleanup()\n",
    "                return\n",
    "\n",
    "            print(\"Converting to Web Mercator projection...\")\n",
    "            self.buildings = self.buildings.to_crs(epsg=3857)\n",
    "            \n",
    "            buffer = max(self.cell_width, self.cell_height)\n",
    "            bounds_box = box(self.x_min - buffer, self.y_min - buffer, \n",
    "                        self.x_max + buffer, self.y_max + buffer)\n",
    "            \n",
    "            self.buildings = self.buildings[self.buildings.geometry.intersects(bounds_box)]\n",
    "            filtered_count = len(self.buildings)\n",
    "            print(f\"Buildings within bounds: {filtered_count}\")\n",
    "            \n",
    "            if filtered_count < min_buildings:\n",
    "                print(f\"Skipping {self.place_name} - only {filtered_count} buildings within bounds (needs {min_buildings}+)\")\n",
    "                self._cleanup()\n",
    "                return\n",
    "                \n",
    "            self._rasterize_buildings()\n",
    "            self._save_data()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing buildings for {self.place_name or 'custom bounding box'}: {e}\")\n",
    "        finally:\n",
    "            self._cleanup()\n",
    "\n",
    "    def _rasterize_buildings(self):\n",
    "        \"\"\"Ultra-fast rasterization using rasterio with error handling\"\"\"\n",
    "        print(\"Performing ultra-fast rasterization with rasterio...\")\n",
    "        \n",
    "        try:\n",
    "            shapes = ((geom, 1) for geom in self.buildings.geometry if geom is not None and geom.is_valid)\n",
    "            \n",
    "            self.grid = rasterize(\n",
    "                shapes=shapes,\n",
    "                out_shape=(self.grid_height, self.grid_width),\n",
    "                transform=self.transform,\n",
    "                dtype=np.uint8,\n",
    "                all_touched=True\n",
    "            )\n",
    "            \n",
    "            building_cells = np.sum(self.grid)\n",
    "            density = building_cells / self.grid.size\n",
    "            print(f\"Rasterized {building_cells} cells ({density:.2%} coverage)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during rasterization for {self.place_name or 'custom bounding box'}: {e}\")\n",
    "            self.grid = np.zeros((self.grid_height, self.grid_width), dtype=np.uint8)\n",
    "\n",
    "    def _save_data(self, output_dir=\"city_data\"):\n",
    "        \"\"\"Save rasterization data to a city-specific subfolder in city_data\"\"\"\n",
    "        if self.grid is None:\n",
    "            print(f\"No grid data to save for {self.place_name or 'custom bounding box'}.\")\n",
    "            return\n",
    "        \n",
    "        safe_name = \"\".join(c for c in (self.place_name or \"CustomArea\") if c.isalnum() or c in (' ', '_')).rstrip().replace(' ', '_')\n",
    "        city_dir = Path(output_dir) / safe_name\n",
    "        city_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        building_cells = np.sum(self.grid)\n",
    "        total_cells = self.grid.size\n",
    "        building_density = building_cells / total_cells\n",
    "        \n",
    "        np.save(city_dir / \"grid.npy\", self.grid)\n",
    "        edge_grid = self.get_edge_grid()\n",
    "        np.save(city_dir / \"edge_grid.npy\", edge_grid)\n",
    "        \n",
    "        metadata = {\n",
    "            \"place_name\": self.place_name,\n",
    "            \"country\": self.country,\n",
    "            \"continent\": self.continent,\n",
    "            \"bounding_box\": {\n",
    "                \"minLon\": self.bounding_box.minLon,\n",
    "                \"maxLon\": self.bounding_box.maxLon,\n",
    "                \"minLat\": self.bounding_box.minLat,\n",
    "                \"maxLat\": self.bounding_box.maxLat\n",
    "            },\n",
    "            \"grid_dimensions\": {\n",
    "                \"width\": self.grid_width,\n",
    "                \"height\": self.grid_height,\n",
    "                \"cell_width_meters\": self.cell_width if self.cell_width else 0.0,\n",
    "                \"cell_height_meters\": self.cell_height if self.cell_height else 0.0\n",
    "            },\n",
    "            \"economic_data\": {\n",
    "                \"gdp_usd_billions\": self.gdp,\n",
    "                \"population\": self.population\n",
    "            },\n",
    "            \"grid_statistics\": {\n",
    "                \"total_cells\": total_cells,\n",
    "                \"building_cells\": int(building_cells),\n",
    "                \"building_density\": float(building_density),\n",
    "                \"edge_cells\": int(np.sum(edge_grid))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(city_dir / \"metadata.json\", 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved data to {city_dir}\")\n",
    "\n",
    "    def _cleanup(self):\n",
    "        \"\"\"Clear large objects to free memory\"\"\"\n",
    "        self.grid = None\n",
    "        self.buildings = None\n",
    "        print(f\"Cleaned up memory for {self.place_name or 'custom bounding box'}\")\n",
    "\n",
    "    def get_edge_grid(self):\n",
    "        \"\"\"Create a grid showing only building edges\"\"\"\n",
    "        if self.grid is None:\n",
    "            raise ValueError(\"No grid data available.\")\n",
    "        \n",
    "        edge_grid = np.zeros_like(self.grid)\n",
    "        \n",
    "        for i in range(1, self.grid.shape[0] - 1):\n",
    "            for j in range(1, self.grid.shape[1] - 1):\n",
    "                if self.grid[i, j] == 1:\n",
    "                    if (self.grid[i-1, j] == 0 or self.grid[i+1, j] == 0 or \n",
    "                        self.grid[i, j-1] == 0 or self.grid[i, j+1] == 0):\n",
    "                        edge_grid[i, j] = 1\n",
    "        \n",
    "        return edge_grid\n",
    "\n",
    "    def save_data(self, output_dir=\"city_data\"):\n",
    "        \"\"\"Save rasterization data to a city-specific subfolder (public method)\"\"\"\n",
    "        self._save_data(output_dir=output_dir)\n",
    "        self._cleanup()\n",
    "\n",
    "    @classmethod\n",
    "    def load_data(cls, city_dir):\n",
    "        \"\"\"Load rasterization data from a city-specific subfolder without re-rasterizing\"\"\"\n",
    "        city_dir = Path(city_dir)\n",
    "        if not city_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory {city_dir} does not exist\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open(city_dir / \"metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        # Create instance without running __init__\n",
    "        instance = cls.__new__(cls)\n",
    "\n",
    "        # Assign metadata fields manually\n",
    "        instance.place_name = metadata.get(\"place_name\")\n",
    "        instance.country = metadata.get(\"country\")\n",
    "        instance.continent = metadata.get(\"continent\")\n",
    "\n",
    "        bbox = metadata[\"bounding_box\"]\n",
    "        instance.bounding_box = BoundingBox(\n",
    "            minLon=bbox[\"minLon\"],\n",
    "            maxLon=bbox[\"maxLon\"],\n",
    "            minLat=bbox[\"minLat\"],\n",
    "            maxLat=bbox[\"maxLat\"]\n",
    "        )\n",
    "\n",
    "        dims = metadata[\"grid_dimensions\"]\n",
    "        instance.grid_width = dims[\"width\"]\n",
    "        instance.grid_height = dims[\"height\"]\n",
    "        instance.cell_width = dims[\"cell_width_meters\"]\n",
    "        instance.cell_height = dims[\"cell_height_meters\"]\n",
    "\n",
    "        econ = metadata[\"economic_data\"]\n",
    "        instance.gdp = econ.get(\"gdp_usd_billions\")\n",
    "        instance.population = econ.get(\"population\")\n",
    "\n",
    "        instance.grid = np.load(city_dir / \"grid.npy\")\n",
    "\n",
    "        print(f\"Loaded pre-rasterized data from {city_dir}\")\n",
    "        return instance\n",
    "\n",
    "    def plot(self, show_edges=False, save_arrays=False, output_dir=\"city_data\"):\n",
    "        \"\"\"\n",
    "        Plot the building distribution with economic data in title\n",
    "        \n",
    "        Args:\n",
    "            show_edges (bool): Whether to show only building edges\n",
    "            save_arrays (bool): Whether to save data (grids, metadata, plots)\n",
    "            output_dir (str): Root directory for saved data\n",
    "        \"\"\"\n",
    "        if self.grid is None:\n",
    "            raise ValueError(\"No grid data available.\")\n",
    "\n",
    "        if save_arrays:\n",
    "            self.save_data(output_dir=output_dir)\n",
    "            return \n",
    "        \n",
    "        plot_grid = self.get_edge_grid() if show_edges else self.grid\n",
    "        title_suffix = \" (Building Edges)\" if show_edges else \"\"\n",
    "        \n",
    "        city_name = self.place_name if self.place_name else \"Custom Area\"\n",
    "        gdp_str = f\"GDP: ${self.gdp:.1f}B\" if self.gdp is not None else \"GDP: N/A\"\n",
    "        pop_str = f\"Pop: {int(self.population):,}\" if self.population is not None else \"Pop: N/A\"\n",
    "        title = f\"Building Distribution in {city_name}\\n{gdp_str}, {pop_str}{title_suffix}\"\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        im = ax.imshow(\n",
    "            plot_grid,\n",
    "            cmap='binary',\n",
    "            interpolation='nearest',\n",
    "            origin='lower',\n",
    "            extent=[self.bounding_box.minLon, self.bounding_box.maxLon,\n",
    "                   self.bounding_box.minLat, self.bounding_box.maxLat]\n",
    "        )\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Longitude\")\n",
    "        ax.set_ylabel(\"Latitude\")\n",
    "        \n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Building Edge Presence' if show_edges else 'Building Presence')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "        \n",
    "        total_cells = plot_grid.size\n",
    "        building_cells = np.sum(plot_grid)\n",
    "        print(f\"Grid statistics for {city_name}:\")\n",
    "        print(f\"  Total cells: {total_cells}\")\n",
    "        print(f\"  Building cells: {building_cells}\")\n",
    "        print(f\"  Building density: {building_cells/total_cells*100:.2f}%\")\n",
    "        print(f\"  GDP: {gdp_str}\")\n",
    "        print(f\"  Population: {pop_str}\")\n",
    "\n",
    "    def get_grid(self):\n",
    "        \"\"\"Return the building grid as numpy array\"\"\"\n",
    "        return self.grid.copy() if self.grid is not None else None\n",
    "\n",
    "    def get_bounds(self):\n",
    "        \"\"\"Return the geographic bounds\"\"\"\n",
    "        return self.bounding_box\n",
    "\n",
    "    def get_economic_data(self):\n",
    "        \"\"\"Return the economic data\"\"\"\n",
    "        return {'gdp': self.gdp, 'population': self.population}\n",
    "\n",
    "class BoundingBox:\n",
    "    def __init__(self, minLon, maxLon, minLat, maxLat):\n",
    "        self.minLon = minLon\n",
    "        self.maxLon = maxLon\n",
    "        self.minLat = minLat\n",
    "        self.maxLat = maxLat\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"BoundingBox(lon: {self.minLon:.6f} to {self.maxLon:.6f}, lat: {self.minLat:.6f} to {self.maxLat:.6f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bfcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityCSVProcessor:\n",
    "    def __init__(self, csv_path='cities.csv', output_dir='city_data', min_population=10000):\n",
    "        \"\"\"\n",
    "        Initialize the CSV processor with path to cities.csv and output directory\n",
    "        \n",
    "        Args:\n",
    "            csv_path (str): Path to the cities.csv file\n",
    "            output_dir (str): Directory to save rasterized city data\n",
    "            min_population (int): Minimum population threshold for processing\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.output_dir = output_dir\n",
    "        self.min_population = min_population\n",
    "        self.df = None\n",
    "        \n",
    "    def load_csv(self):\n",
    "        \"\"\"Load and preprocess the cities.csv file\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_path, sep=';')\n",
    "            print(f\"Successfully loaded {len(self.df)} cities from {self.csv_path}\")\n",
    "            \n",
    "            self.df['Coordinates'] = self.df['Coordinates'].str.strip()\n",
    "            self.df = self.df[self.df['Population'] > self.min_population]\n",
    "            print(f\"Found {len(self.df)} cities with population > {self.min_population}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file {self.csv_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_all_cities(self, grid_width=3000, grid_height=3000):\n",
    "        \"\"\"\n",
    "        Process all cities in the CSV file and rasterize them using OSM boundaries\n",
    "        \n",
    "        Args:\n",
    "            grid_width (int): Number of cells in x-direction\n",
    "            grid_height (int): Number of cells in y-direction\n",
    "        \"\"\"\n",
    "        if not self.load_csv():\n",
    "            return\n",
    "        \n",
    "        processed_cities = self.get_processed_cities()\n",
    "        processed_place_names = set(city['place_name'] for city in processed_cities)\n",
    "        print(f\"Found {len(processed_place_names)} already processed cities in {self.output_dir}\")\n",
    "        \n",
    "        success_count = 0\n",
    "        skip_count = 0\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                city_name = row['Name']\n",
    "                country = row['Country name EN']\n",
    "                population = row['Population']\n",
    "                place_name = f\"{city_name}, {country}\"\n",
    "                \n",
    "                if place_name in processed_place_names:\n",
    "                    print(f\"Skipping {place_name} - already processed\")\n",
    "                    skip_count += 1\n",
    "                    continue\n",
    "                \n",
    "                print(f\"\\nProcessing {idx+1}/{len(self.df)}: {place_name}\")\n",
    "                print(f\"  Population: {population:,}\")\n",
    "                \n",
    "                # Let CityRasterizer handle the bounding box automatically\n",
    "                rasterizer = CityRasterizer(\n",
    "                    place_name=place_name,\n",
    "                    country=country,\n",
    "                    grid_width=grid_width,\n",
    "                    grid_height=grid_height,\n",
    "                    population=population\n",
    "                )\n",
    "                \n",
    "                rasterizer.save_data(output_dir=self.output_dir)\n",
    "                success_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row['Name']}, {row['Country name EN']}: {e}\")\n",
    "                skip_count += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nFinished. Newly processed: {success_count}, Skipped: {skip_count}\")\n",
    "    \n",
    "    def get_processed_cities(self):\n",
    "        \"\"\"Return a list of all successfully processed cities\"\"\"\n",
    "        processed = []\n",
    "        city_dirs = Path(self.output_dir).glob('*')\n",
    "        \n",
    "        for city_dir in city_dirs:\n",
    "            if city_dir.is_dir():\n",
    "                metadata_path = city_dir / 'metadata.json'\n",
    "                if metadata_path.exists():\n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                        processed.append({\n",
    "                            'place_name': metadata['place_name'],\n",
    "                            'country': metadata['country'],\n",
    "                            'path': str(city_dir)\n",
    "                        })\n",
    "        \n",
    "        return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = CityCSVProcessor(\n",
    "    csv_path='cities.csv',\n",
    "    output_dir='city_data'\n",
    ")\n",
    "\n",
    "# Process with higher resolution grid\n",
    "processor.process_all_cities(grid_width=3000, grid_height=3000)\n",
    "\n",
    "# Verify processing\n",
    "processed = processor.get_processed_cities()\n",
    "print(f\"Successfully processed {len(processed)} cities at higher resolution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Enable mixed precision for memory efficiency\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "class CityDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Generator for 1024x1024 resolution images (resized from 3000x3000)\"\"\"\n",
    "    def __init__(self, city_dirs, metadata_df, batch_size=20, image_size=(1024, 1024),\n",
    "                 shuffle=True, mode='train', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.city_dirs = city_dirs\n",
    "        self.metadata_df = metadata_df\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.country_encoder = LabelEncoder()\n",
    "        valid_countries = metadata_df['Country'].dropna().unique()\n",
    "        self.country_encoder.fit(valid_countries)\n",
    "        \n",
    "        self.pop_scaler = StandardScaler()\n",
    "        valid_populations = metadata_df[['Population']].dropna()\n",
    "        self.pop_scaler.fit(valid_populations)\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.city_dirs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_dirs = self.city_dirs[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        X_list = []\n",
    "        y_country_list = []\n",
    "        y_population_list = []\n",
    "        \n",
    "        for city_dir in batch_dirs:\n",
    "            grid_path = city_dir / \"grid.npy\"\n",
    "            if not grid_path.exists():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                img = np.load(grid_path).astype('float32')\n",
    "                \n",
    "                if img.shape != (3000, 3000):\n",
    "                    continue\n",
    "                \n",
    "                img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "                \n",
    "                img_tensor = tf.convert_to_tensor(img[..., np.newaxis], dtype=tf.float32)\n",
    "                resized_img = tf.image.resize(img_tensor, self.image_size, method='bilinear')\n",
    "                \n",
    "                city_name = city_dir.name.replace('_', ', ')\n",
    "                city_meta = self.metadata_df[self.metadata_df['place_name'] == city_name]\n",
    "                if city_meta.empty:\n",
    "                    continue\n",
    "                city_meta = city_meta.iloc[0]\n",
    "                \n",
    "                country = city_meta['Country']\n",
    "                if country not in self.country_encoder.classes_:\n",
    "                    continue\n",
    "                \n",
    "                country_encoded = self.country_encoder.transform([country])[0]\n",
    "                population_scaled = self.pop_scaler.transform(pd.DataFrame({'Population': [city_meta['Population']]}))[0]\n",
    "                \n",
    "                X_list.append(resized_img.numpy())\n",
    "                y_country_list.append(country_encoded)\n",
    "                y_population_list.append(population_scaled)\n",
    "            \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if not X_list:\n",
    "            X = np.zeros((1, *self.image_size, 1), dtype=np.float32)\n",
    "            y_country = np.zeros((1, len(self.country_encoder.classes_)), dtype=np.float32)\n",
    "            y_population = np.zeros((1, 1), dtype=np.float32)\n",
    "            return X, {'country_output': y_country, 'population_output': y_population, 'reconstruction': X}\n",
    "        \n",
    "        X = np.array(X_list, dtype=np.float32)\n",
    "        y_country = np.array(y_country_list, dtype=int)\n",
    "        y_population = np.array(y_population_list, dtype=np.float32)\n",
    "        \n",
    "        y_country = keras.utils.to_categorical(y_country, num_classes=len(self.country_encoder.classes_))\n",
    "        \n",
    "        return X, {'country_output': y_country, 'population_output': y_population, 'reconstruction': X}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.city_dirs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            self.city_dirs = [self.city_dirs[i] for i in self.indexes]\n",
    "\n",
    "class ProgressiveAutoencoderModel:\n",
    "    def __init__(self, num_countries, input_shape=(1024, 1024, 1)):\n",
    "        self.num_countries = num_countries\n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def build_model(self):\n",
    "        inputs = keras.Input(shape=self.input_shape)\n",
    "        \n",
    "        x = layers.Conv2D(16, (3, 3), strides=2, activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = self._downsample_block(x, 32, strides=2)\n",
    "        x = self._downsample_block(x, 64, strides=2)\n",
    "        x = self._downsample_block(x, 128, strides=2)\n",
    "        x = self._downsample_block(x, 256, strides=2)\n",
    "        x = self._downsample_block(x, 512, strides=2)\n",
    "        \n",
    "        encoded = x\n",
    "        \n",
    "        pooled = layers.GlobalAveragePooling2D(name='global_avg_pool')(encoded)\n",
    "        \n",
    "        country_output = layers.Dense(\n",
    "            self.num_countries, \n",
    "            activation='softmax',\n",
    "            name='country_output',\n",
    "            dtype='float32' \n",
    "        )(pooled)\n",
    "        \n",
    "        population_output = layers.Dense(\n",
    "            1,\n",
    "            name='population_output',\n",
    "            dtype='float32'\n",
    "        )(pooled)\n",
    "        \n",
    "        x = self._upsample_block(encoded, 256, strides=2)\n",
    "        x = self._upsample_block(x, 128, strides=2)\n",
    "        x = self._upsample_block(x, 64, strides=2)\n",
    "        x = self._upsample_block(x, 32, strides=2)\n",
    "        x = self._upsample_block(x, 16, strides=2)\n",
    "        x = self._upsample_block(x, 16, strides=2)\n",
    "        \n",
    "        reconstruction = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='reconstruction')(x)\n",
    "        \n",
    "        model = keras.Model(inputs=inputs, outputs=[country_output, population_output, reconstruction])\n",
    "        return model\n",
    "    \n",
    "    def _downsample_block(self, x, filters, strides):\n",
    "        x = layers.Conv2D(filters, (3, 3), strides=strides, \n",
    "                         activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        return x\n",
    "    \n",
    "    def _upsample_block(self, x, filters, strides):\n",
    "        x = layers.Conv2DTranspose(filters, (3, 3), strides=strides, \n",
    "                                   activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        return x\n",
    "\n",
    "def prepare_datasets(data_dir=\"city_data\", test_size=0.2, val_size=0.1, batch_size=4):\n",
    "    data_dir = Path(data_dir)\n",
    "    city_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "    metadata = []\n",
    "    \n",
    "    for city_dir in city_dirs:\n",
    "        try:\n",
    "            meta_path = city_dir / \"metadata.json\"\n",
    "            if not meta_path.exists():\n",
    "                continue\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                population = meta['economic_data']['population']\n",
    "                building_density = meta['grid_statistics']['building_density']\n",
    "                if population is None or building_density is None:\n",
    "                    continue\n",
    "                if population <= 10000 or building_density <= 0.005:\n",
    "                    continue\n",
    "                metadata.append({\n",
    "                    'place_name': meta['place_name'],\n",
    "                    'Country': meta['country'],\n",
    "                    'Population': population,\n",
    "                    'Building_Density': building_density\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df = metadata_df.dropna(subset=['Country', 'Population', 'Building_Density'])\n",
    "    \n",
    "    valid_cities = metadata_df['place_name'].values\n",
    "    city_dirs = [d for d in city_dirs if d.name.replace('_', ', ') in valid_cities]\n",
    "    \n",
    "    train_dirs, test_dirs = train_test_split(city_dirs, test_size=test_size, random_state=42)\n",
    "    train_dirs, val_dirs = train_test_split(train_dirs, test_size=val_size/(1-test_size), random_state=42)\n",
    "    \n",
    "    train_gen = CityDataGenerator(\n",
    "        train_dirs, metadata_df, \n",
    "        batch_size=batch_size,\n",
    "        image_size=(1024, 1024),\n",
    "        mode='train'\n",
    "    )\n",
    "    val_gen = CityDataGenerator(\n",
    "        val_dirs, metadata_df,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(1024, 1024),\n",
    "        mode='val'\n",
    "    )\n",
    "    test_gen = CityDataGenerator(\n",
    "        test_dirs, metadata_df,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(1024, 1024),\n",
    "        mode='test'\n",
    "    )\n",
    "    \n",
    "    return train_gen, val_gen, test_gen, metadata_df\n",
    "\n",
    "def visualize_latent_embeddings(model, test_gen, num_samples=100):\n",
    "    \"\"\"Visualize latent embeddings using t-SNE\"\"\"\n",
    "    embedding_model = keras.Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=model.get_layer('global_avg_pool').output\n",
    "    )\n",
    "    \n",
    "    embeddings = []\n",
    "    country_labels = []\n",
    "    city_names = []\n",
    "    \n",
    "    samples_collected = 0\n",
    "    for i in range(len(test_gen)):\n",
    "        X, y = test_gen[i]\n",
    "        batch_embeddings = embedding_model.predict(X, verbose=0)\n",
    "        batch_countries = np.argmax(y['country_output'], axis=1)\n",
    "        \n",
    "        for j in range(len(X)):\n",
    "            if samples_collected >= num_samples:\n",
    "                break\n",
    "            embeddings.append(batch_embeddings[j])\n",
    "            country_labels.append(batch_countries[j])\n",
    "            city_idx = test_gen.indexes[i * test_gen.batch_size + j]\n",
    "            city_name = test_gen.city_dirs[city_idx].name.replace('_', ', ')\n",
    "            city_names.append(city_name)\n",
    "            samples_collected += 1\n",
    "        \n",
    "        if samples_collected >= num_samples:\n",
    "            break\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    country_labels = test_gen.country_encoder.inverse_transform(country_labels)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_countries = np.unique(country_labels)\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_countries)))\n",
    "    color_map = dict(zip(unique_countries, colors))\n",
    "    \n",
    "    for country in unique_countries:\n",
    "        mask = country_labels == country\n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            label=country,\n",
    "            c=[color_map[country]],\n",
    "            alpha=0.6\n",
    "        )\n",
    "    \n",
    "    plt.title(\"Latent Embeddings of City Grids (t-SNE)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Example embeddings:\")\n",
    "    for i in range(min(5, len(city_names))):\n",
    "        print(f\"City: {city_names[i]}, Country: {country_labels[i]}\")\n",
    "\n",
    "def visualize_reconstructions(model, test_gen, num_samples=5):\n",
    "    \"\"\"Visualize original vs reconstructed images to see learned structures\"\"\"\n",
    "    originals = []\n",
    "    reconstructions = []\n",
    "    city_names = []\n",
    "    \n",
    "    samples_collected = 0\n",
    "    for i in range(len(test_gen)):\n",
    "        X, y = test_gen[i]\n",
    "        preds = model.predict(X, verbose=0)\n",
    "        batch_recons = preds[2]\n",
    "        \n",
    "        for j in range(len(X)):\n",
    "            if samples_collected >= num_samples:\n",
    "                break\n",
    "            originals.append(X[j].squeeze())\n",
    "            reconstructions.append(batch_recons[j].squeeze())\n",
    "            city_idx = test_gen.indexes[i * test_gen.batch_size + j]\n",
    "            city_name = test_gen.city_dirs[city_idx].name.replace('_', ', ')\n",
    "            city_names.append(city_name)\n",
    "            samples_collected += 1\n",
    "        \n",
    "        if samples_collected >= num_samples:\n",
    "            break\n",
    "    \n",
    "    n = len(originals)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(originals[i], cmap='viridis')\n",
    "        plt.title(f\"Original: {city_names[i]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructions[i], cmap='viridis')\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_and_resize_images(data_dir=\"city_data\", image_size=(1024, 1024), num_images=3):\n",
    "    \"\"\"Load and resize the first num_images valid grid.npy files.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    city_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    images = []\n",
    "    city_names = []\n",
    "    count = 0\n",
    "    \n",
    "    for city_dir in city_dirs:\n",
    "        if count >= num_images:\n",
    "            break\n",
    "            \n",
    "        grid_path = city_dir / \"grid.npy\"\n",
    "        if not grid_path.exists():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            img = np.load(grid_path).astype('float32')\n",
    "            if img.shape != (3000, 3000):\n",
    "                continue\n",
    "                \n",
    "            img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "            \n",
    "            img_tensor = tf.convert_to_tensor(img[..., np.newaxis], dtype=tf.float32)\n",
    "            resized_img = tf.image.resize(img_tensor, image_size, method='bilinear')\n",
    "            resized_img = resized_img.numpy().squeeze()\n",
    "            \n",
    "            images.append(resized_img)\n",
    "            city_names.append(city_dir.name.replace('_', ', '))\n",
    "            count += 1\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return images, city_names\n",
    "\n",
    "def plot_images(images, city_names):\n",
    "    \"\"\"Plot the images with city names as titles.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, (img, name) in enumerate(zip(images, city_names)):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.imshow(img, cmap='viridis')\n",
    "        plt.title(name, fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model():\n",
    "    train_gen, val_gen, test_gen, metadata_df = prepare_datasets(batch_size=4)\n",
    "    \n",
    "    if len(train_gen.city_dirs) == 0 or len(val_gen.city_dirs) == 0:\n",
    "        raise ValueError(\"No valid training or validation data found. Check city_data directory and metadata.\")\n",
    "    \n",
    "    num_countries = len(metadata_df['Country'].unique())\n",
    "    model_builder = ProgressiveAutoencoderModel(num_countries, input_shape=(1024, 1024, 1))\n",
    "    model = model_builder.build_model()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss={\n",
    "            'country_output': 'categorical_crossentropy',\n",
    "            'population_output': 'mse',\n",
    "            'reconstruction': 'mse'\n",
    "        },\n",
    "        metrics={\n",
    "            'country_output': 'accuracy',\n",
    "            'population_output': ['mae', tf.keras.metrics.RootMeanSquaredError()],\n",
    "            'reconstruction': ['mae']\n",
    "        },\n",
    "        loss_weights={\n",
    "            'country_output': 0.5,\n",
    "            'population_output': 0.05,\n",
    "            'reconstruction': 1.0\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_1024_autoencoder.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min'\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    test_results = model.evaluate(test_gen, verbose=1)\n",
    "    print(f\"Test results - Loss: {test_results[0]}, Country Accuracy: {test_results[4]}, Population MAE: {test_results[5]}, Reconstruction MAE: {test_results[6]}\")\n",
    "    \n",
    "    return model, history, test_gen\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        images, city_names = load_and_resize_images(data_dir=\"city_data\", image_size=(1024, 1024), num_images=3)\n",
    "        if not images:\n",
    "            print(\"No valid images found in the city_data directory.\")\n",
    "        else:\n",
    "            print(f\"Displaying {len(images)} resized images:\")\n",
    "            plot_images(images, city_names)\n",
    "        \n",
    "        model, history, test_gen = train_model()\n",
    "        visualize_latent_embeddings(model, test_gen, num_samples=100)\n",
    "        visualize_reconstructions(model, test_gen, num_samples=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143679b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training vs validation loss and accuracy from model history.\"\"\"\n",
    "    hist = history.history\n",
    "    \n",
    "    # --- Loss curves ---\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(hist['loss'], label='Training Loss')\n",
    "    plt.plot(hist['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Country accuracy curves (classification branch only) ---\n",
    "    if 'country_output_accuracy' in hist and 'val_country_output_accuracy' in hist:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(hist['country_output_accuracy'], label='Training Accuracy')\n",
    "        plt.plot(hist['val_country_output_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Country Classification Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_all_embeddings(model, data_gen):\n",
    "    \"\"\"Extract latent embeddings + metadata for all cities in a generator.\"\"\"\n",
    "    embedding_model = keras.Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=model.get_layer('global_avg_pool').output\n",
    "    )\n",
    "    \n",
    "    embeddings = []\n",
    "    city_names = []\n",
    "    countries = []\n",
    "    originals = []\n",
    "    reconstructions = []\n",
    "\n",
    "    for i in range(len(data_gen)):\n",
    "        X, y = data_gen[i]\n",
    "        emb = embedding_model.predict(X, verbose=0)\n",
    "        preds = model.predict(X, verbose=0)\n",
    "        recons = preds[2]\n",
    "\n",
    "        embeddings.append(emb)\n",
    "        countries.extend(test_gen.country_encoder.inverse_transform(np.argmax(y['country_output'], axis=1)))\n",
    "        city_names.extend([d.name.replace('_', ', ') for d in data_gen.city_dirs[i*data_gen.batch_size:(i+1)*data_gen.batch_size]])\n",
    "        originals.extend(X)\n",
    "        reconstructions.extend(recons)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    originals = np.array(originals)\n",
    "    reconstructions = np.array(reconstructions)\n",
    "\n",
    "    return embeddings, np.array(city_names), np.array(countries), originals, reconstructions\n",
    "\n",
    "def run_kmeans_on_embeddings(embeddings, countries, n_clusters=2):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Country\": countries,\n",
    "        \"Cluster\": cluster_labels\n",
    "    })\n",
    "    print(df.groupby([\"Cluster\", \"Country\"]).size().unstack(fill_value=0))\n",
    "    return cluster_labels\n",
    "\n",
    "def visualize_clusters(originals, reconstructions, city_names, cluster_labels, n_samples=5):\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    for cluster in unique_clusters:\n",
    "        idxs = np.where(cluster_labels == cluster)[0][:n_samples]\n",
    "        \n",
    "        print(f\"\\nCluster {cluster} examples:\")\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        for j, idx in enumerate(idxs):\n",
    "            # Original\n",
    "            ax = plt.subplot(2, n_samples, j+1)\n",
    "            plt.imshow(originals[idx].squeeze(), cmap='viridis')\n",
    "            plt.title(city_names[idx], fontsize=10)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Reconstruction\n",
    "            ax = plt.subplot(2, n_samples, j+1+n_samples)\n",
    "            plt.imshow(reconstructions[idx].squeeze(), cmap='viridis')\n",
    "            plt.title(\"Reconstructed\", fontsize=10)\n",
    "            plt.axis(\"off\")\n",
    "        plt.suptitle(f\"Cluster {cluster}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b902393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow import keras\n",
    "\n",
    "def prepare_datasets_all(data_dir=\"city_data\", batch_size=4):\n",
    "    \"\"\"Prepare a single dataset generator for all valid cities in city_data.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    city_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "    metadata = []\n",
    "    \n",
    "    for city_dir in city_dirs:\n",
    "        try:\n",
    "            meta_path = city_dir / \"metadata.json\"\n",
    "            if not meta_path.exists():\n",
    "                continue\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                population = meta['economic_data']['population']\n",
    "                building_density = meta['grid_statistics']['building_density']\n",
    "                if population is None or building_density is None:\n",
    "                    continue\n",
    "                metadata.append({\n",
    "                    'place_name': meta['place_name'],\n",
    "                    'Country': meta['country'],\n",
    "                    'Population': population,\n",
    "                    'Building_Density': building_density\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df = metadata_df.dropna(subset=['Country', 'Population', 'Building_Density'])\n",
    "    \n",
    "    valid_cities = metadata_df['place_name'].values\n",
    "    city_dirs = [d for d in city_dirs if d.name.replace('_', ', ') in valid_cities]\n",
    "    \n",
    "    all_gen = CityDataGenerator(\n",
    "        city_dirs, metadata_df,\n",
    "        batch_size=batch_size,\n",
    "        image_size=(1024, 1024),\n",
    "        mode='all',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return all_gen, metadata_df\n",
    "\n",
    "def get_all_embeddings(model, all_gen, num_samples=None):\n",
    "    \"\"\"\n",
    "    Extract embeddings, city names, countries, originals, and reconstructions for all cities.\n",
    "    If num_samples is specified, limit to that number; otherwise, process all.\n",
    "    \"\"\"\n",
    "    embedding_model = keras.Model(\n",
    "        inputs=model.inputs,\n",
    "        outputs=model.get_layer('global_avg_pool').output\n",
    "    )\n",
    "    \n",
    "    embeddings = []\n",
    "    city_names = []\n",
    "    countries = []\n",
    "    originals = []\n",
    "    reconstructions = []\n",
    "    \n",
    "    samples_collected = 0\n",
    "    max_samples = num_samples if num_samples is not None else len(all_gen.city_dirs)\n",
    "    \n",
    "    for i in range(len(all_gen)):\n",
    "        X, y = all_gen[i]\n",
    "        batch_embeddings = embedding_model.predict(X, verbose=0)\n",
    "        batch_recons = model.predict(X, verbose=0)[2]\n",
    "        batch_countries = np.argmax(y['country_output'], axis=1)\n",
    "        \n",
    "        for j in range(len(X)):\n",
    "            if samples_collected >= max_samples:\n",
    "                break\n",
    "            embeddings.append(batch_embeddings[j])\n",
    "            originals.append(X[j].squeeze())\n",
    "            reconstructions.append(batch_recons[j].squeeze())\n",
    "            city_idx = all_gen.indexes[i * all_gen.batch_size + j]\n",
    "            city_name = all_gen.city_dirs[city_idx].name.replace('_', ', ')\n",
    "            city_names.append(city_name)\n",
    "            countries.append(all_gen.country_encoder.inverse_transform([batch_countries[j]])[0])\n",
    "            samples_collected += 1\n",
    "        \n",
    "        if samples_collected >= max_samples:\n",
    "            break\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    originals = np.array(originals)\n",
    "    reconstructions = np.array(reconstructions)\n",
    "    city_names = np.array(city_names)\n",
    "    countries = np.array(countries)\n",
    "    \n",
    "    return embeddings, city_names, countries, originals, reconstructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "501db477",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gen, metadata_df = prepare_datasets_all(batch_size=4)\n",
    "\n",
    "embeddings, city_names, countries, originals, reconstructions = get_all_embeddings(\n",
    "    model, all_gen, num_samples=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f59158e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4638"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb322a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"oecd_cities.csv\")\n",
    "\n",
    "gdp_df = df[df[\"VAR\"] == \"GDP_PC_REAL_PPP\"].copy()\n",
    "\n",
    "gdp_df = gdp_df[[\"Metropolitan areas\", \"TIME_PERIOD\", \"OBS_VALUE\"]]\n",
    "gdp_df.rename(columns={\n",
    "    \"Metropolitan areas\": \"City\",\n",
    "    \"TIME_PERIOD\": \"Year\",\n",
    "    \"OBS_VALUE\": \"GDP_per_capita\"\n",
    "}, inplace=True)\n",
    "\n",
    "gdp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61583f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import pearsonr, spearmanr, f_oneway, kruskal\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "from collections import Counter\n",
    "\n",
    "def load_oecd_gdp_data(csv_path=\"oecd_cities.csv\"):\n",
    "    \"\"\"Load OECD GDP data and return as dictionary for fast lookup\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        gdp_df = df[df[\"VAR\"] == \"GDP_PC_REAL_PPP\"].copy()\n",
    "        gdp_df = gdp_df[[\"Metropolitan areas\", \"TIME_PERIOD\", \"OBS_VALUE\"]]\n",
    "        gdp_df.rename(columns={\n",
    "            \"Metropolitan areas\": \"City\",\n",
    "            \"TIME_PERIOD\": \"Year\", \n",
    "            \"OBS_VALUE\": \"GDP_per_capita\"\n",
    "        }, inplace=True)\n",
    "        gdp_df = gdp_df.loc[gdp_df.groupby('City')['Year'].idxmax()]\n",
    "        return dict(zip(gdp_df['City'].str.strip().str.lower(), gdp_df['GDP_per_capita']))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {csv_path} not found\")\n",
    "        return {}\n",
    "\n",
    "def get_population_from_metadata(city_name, data_dir=\"city_data\"):\n",
    "    \"\"\"Get population for a single city from metadata\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    city_dir = data_dir / city_name.replace(', ', '_')\n",
    "    meta_path = city_dir / \"metadata.json\"\n",
    "    try:\n",
    "        with open(meta_path, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        population = meta['economic_data']['population']\n",
    "        return population if population is not None else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_building_density(city_name, data_dir=\"city_data\"):\n",
    "    \"\"\"Get building_density for a single city from metadata\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    city_dir = data_dir / city_name.replace(', ', '_')\n",
    "    meta_path = city_dir / \"metadata.json\"\n",
    "    try:\n",
    "        with open(meta_path, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        density = meta['grid_statistics']['building_density']\n",
    "        return density if density is not None else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_gdp_for_cities(city_names, countries, oecd_gdp_dict):\n",
    "    \"\"\"Get GDP data for cities using OECD data and country fallbacks\"\"\"\n",
    "    gdp_values = []\n",
    "    print(\"Fetching country-level GDP data...\")\n",
    "    try:\n",
    "        url = \"https://api.worldbank.org/v2/country/all/indicator/NY.GDP.PCAP.PP.KD?format=json&date=2023&per_page=300\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()[1]\n",
    "        country_gdp = {entry['country']['value'].lower(): entry['value'] \n",
    "                       for entry in data if entry['value'] is not None}\n",
    "    except:\n",
    "        print(\"Warning: Could not fetch country GDP data\")\n",
    "        country_gdp = {}\n",
    "    \n",
    "    for city, country in zip(city_names, countries):\n",
    "        city_clean = city.replace(',.*', '').strip().lower()\n",
    "        if city_clean in oecd_gdp_dict:\n",
    "            gdp_values.append(oecd_gdp_dict[city_clean])\n",
    "        else:\n",
    "            matches = [(k, fuzz.token_sort_ratio(city_clean, k)) for k in oecd_gdp_dict.keys()]\n",
    "            best_match = max(matches, key=lambda x: x[1]) if matches else (None, 0)\n",
    "            if best_match[1] >= 80:\n",
    "                gdp_values.append(oecd_gdp_dict[best_match[0]])\n",
    "                print(f\"Fuzzy matched {city} to {best_match[0]} (score: {best_match[1]})\")\n",
    "            else:\n",
    "                if country.lower() in country_gdp:\n",
    "                    gdp_values.append(country_gdp[country.lower()])\n",
    "                    print(f\"Using country GDP for {city} ({country})\")\n",
    "                else:\n",
    "                    gdp_values.append(np.nan)\n",
    "                    print(f\"No GDP data found for {city} ({country})\")\n",
    "    return np.array(gdp_values, dtype=np.float64)\n",
    "\n",
    "def validate_inputs(embeddings, city_names, countries, originals, reconstructions, n_clusters):\n",
    "    \"\"\"Validate input variables and check for numerical issues\"\"\"\n",
    "    if not all([embeddings is not None, city_names is not None, countries is not None, \n",
    "                originals is not None, reconstructions is not None]):\n",
    "        raise NameError(\"One or more input variables are not defined\")\n",
    "    if len(embeddings) != len(city_names) or len(embeddings) != len(countries) or \\\n",
    "       len(embeddings) != len(originals) or len(embeddings) != len(reconstructions):\n",
    "        raise ValueError(\"Input arrays must have the same length\")\n",
    "    if n_clusters < 2 or n_clusters > len(embeddings):\n",
    "        raise ValueError(f\"n_clusters must be between 2 and {len(embeddings)}\")\n",
    "    \n",
    "    if np.any(~np.isfinite(embeddings)):\n",
    "        raise ValueError(\"Embeddings contain NaN or infinite values\")\n",
    "    if np.any(np.abs(embeddings) > 1e15):\n",
    "        print(\"Warning: Embeddings contain very large values, converting to float64\")\n",
    "        embeddings = embeddings.astype(np.float64)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def cluster_cities_memory_efficient(embeddings, city_names, countries, originals, reconstructions, \n",
    "                                  n_clusters=7, data_dir=\"city_data\"):\n",
    "    \"\"\"Perform clustering with filter for building_density >= 0.04\"\"\"\n",
    "    embeddings = validate_inputs(embeddings, city_names, countries, originals, reconstructions, n_clusters)\n",
    "    print(f\"Starting analysis for {len(city_names)} cities...\")\n",
    "    \n",
    "    # Load data\n",
    "    populations = np.array([get_population_from_metadata(city, data_dir) for city in city_names], dtype=np.float64)\n",
    "    building_densities = np.array([get_building_density(city, data_dir) for city in city_names], dtype=np.float64)\n",
    "    oecd_gdp_dict = load_oecd_gdp_data()\n",
    "    gdp_values = get_gdp_for_cities(city_names, countries, oecd_gdp_dict)\n",
    "    \n",
    "    # Check for large or infinite values\n",
    "    for name, arr in [(\"populations\", populations), (\"building_densities\", building_densities), (\"gdp_values\", gdp_values)]:\n",
    "        if np.any(~np.isfinite(arr)):\n",
    "            print(f\"Warning: {name} contains NaN or infinite values\")\n",
    "        if np.any(np.abs(arr) > 1e15):\n",
    "            print(f\"Warning: {name} contains very large values, converting to float64\")\n",
    "            if name == \"populations\":\n",
    "                populations = arr.astype(np.float64)\n",
    "            elif name == \"building_densities\":\n",
    "                building_densities = arr.astype(np.float64)\n",
    "            elif name == \"gdp_values\":\n",
    "                gdp_values = arr.astype(np.float64)\n",
    "    \n",
    "    # Diagnostic: Print ranges\n",
    "    print(f\"Embeddings range: min={np.min(embeddings):.2e}, max={np.max(embeddings):.2e}\")\n",
    "    print(f\"Populations range: min={np.min(populations):.2e}, max={np.max(populations):.2e}\")\n",
    "    print(f\"Building density range: min={np.min(building_densities):.2e}, max={np.max(building_densities):.2e}\")\n",
    "    print(f\"GDP values range: min={np.min(gdp_values):.2e}, max={np.max(gdp_values):.2e}\")\n",
    "    \n",
    "    # Filter for valid data and building_density >= 0.04\n",
    "    valid_mask = ~(np.isnan(populations) | np.isinf(populations) | \n",
    "                   np.isnan(gdp_values) | np.isinf(gdp_values) | \n",
    "                   np.isnan(building_densities) | np.isinf(building_densities)) & (building_densities >= 0.001)\n",
    "    \n",
    "    print(f\"Cities with valid population, GDP, and building_density >= 0.04: {np.sum(valid_mask)} out of {len(city_names)}\")\n",
    "    \n",
    "    # Print invalid cities for debugging\n",
    "    invalid_mask = ~valid_mask\n",
    "    if np.any(invalid_mask):\n",
    "        print(\"Cities with invalid data or building_density < 0.04:\")\n",
    "        for i in np.where(invalid_mask)[0]:\n",
    "            print(f\"  {city_names[i]}: pop={populations[i]}, gdp={gdp_values[i]}, density={building_densities[i]}\")\n",
    "    \n",
    "    valid_embeddings = embeddings[valid_mask]\n",
    "    valid_city_names = [city_names[i] for i in np.where(valid_mask)[0]]\n",
    "    valid_countries = [countries[i] for i in np.where(valid_mask)[0]]\n",
    "    valid_populations = populations[valid_mask]\n",
    "    valid_gdp = gdp_values[valid_mask]\n",
    "    valid_originals = [originals[i] for i in np.where(valid_mask)[0]]\n",
    "    valid_reconstructions = [reconstructions[i] for i in np.where(valid_mask)[0]]\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(valid_city_names)} cities\")\n",
    "    \n",
    "    densities, avg_sizes, num_components = extract_urban_metrics(valid_originals)\n",
    "    \n",
    "    urban_valid_mask = np.isfinite(densities) & np.isfinite(avg_sizes) & ~np.isinf(densities) & ~np.isinf(avg_sizes)\n",
    "    print(f\"Cities with valid urban metrics: {np.sum(urban_valid_mask)} out of {len(valid_city_names)}\")\n",
    "    \n",
    "    final_embeddings = valid_embeddings[urban_valid_mask].astype(np.float64)\n",
    "    final_city_names = [valid_city_names[i] for i in np.where(urban_valid_mask)[0]]\n",
    "    final_countries = [valid_countries[i] for i in np.where(urban_valid_mask)[0]]\n",
    "    final_populations = valid_populations[urban_valid_mask].astype(np.float64)\n",
    "    final_gdp = valid_gdp[urban_valid_mask].astype(np.float64)\n",
    "    final_originals = [valid_originals[i] for i in np.where(urban_valid_mask)[0]]\n",
    "    final_reconstructions = [valid_reconstructions[i] for i in np.where(urban_valid_mask)[0]]\n",
    "    densities = densities[urban_valid_mask].astype(np.float64)\n",
    "    avg_sizes = avg_sizes[urban_valid_mask].astype(np.float64)\n",
    "    num_components = num_components[urban_valid_mask].astype(np.float64)\n",
    "    \n",
    "    if np.any(~np.isfinite(final_embeddings)):\n",
    "        raise ValueError(\"Final embeddings contain NaN or infinite values\")\n",
    "    \n",
    "    features_to_cluster = np.column_stack([final_embeddings, final_populations.reshape(-1, 1), \n",
    "                                          densities.reshape(-1, 1), avg_sizes.reshape(-1, 1)])\n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        features_scaled = scaler.fit_transform(features_to_cluster.astype(np.float64))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in StandardScaler: {e}\")\n",
    "        print(f\"Features range: min={np.min(features_to_cluster, axis=0)}, max={np.max(features_to_cluster, axis=0)}\")\n",
    "        raise\n",
    "    \n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    \n",
    "    original_indices = np.where(valid_mask)[0][urban_valid_mask]\n",
    "    \n",
    "    return (original_indices, cluster_labels, final_city_names, final_countries, \n",
    "            final_populations, final_gdp, final_originals, final_reconstructions, \n",
    "            features_scaled, kmeans, final_embeddings)\n",
    "\n",
    "def analyze_correlations(embeddings, populations, gdp_values):\n",
    "    \"\"\"Compute Pearson, Spearman, and partial correlations\"\"\"\n",
    "    correlations = {'pearson': {}, 'spearman': {}, 'partial': {}}\n",
    "    \n",
    "    if np.any(~np.isfinite(populations)) or np.any(~np.isfinite(gdp_values)):\n",
    "        raise ValueError(\"Populations or GDP values contain NaN or infinite values\")\n",
    "    \n",
    "    pearson_pop_gdp, p_pop_gdp = pearsonr(populations, gdp_values)\n",
    "    spearman_pop_gdp, sp_pop_gdp = spearmanr(populations, gdp_values)\n",
    "    correlations['pearson']['population_gdp'] = (pearson_pop_gdp, p_pop_gdp)\n",
    "    correlations['spearman']['population_gdp'] = (spearman_pop_gdp, sp_pop_gdp)\n",
    "    \n",
    "    for i in range(embeddings.shape[1]):\n",
    "        pearson_corr, p_value = pearsonr(embeddings[:, i], gdp_values)\n",
    "        spearman_corr, sp_value = spearmanr(embeddings[:, i], gdp_values)\n",
    "        correlations['pearson'][f'embedding_{i}'] = (pearson_corr, p_value)\n",
    "        correlations['spearman'][f'embedding_{i}'] = (spearman_corr, sp_value)\n",
    "    \n",
    "    df = pd.DataFrame(np.column_stack([embeddings, populations, gdp_values]), \n",
    "                      columns=[f'emb_{i}' for i in range(embeddings.shape[1])] + ['population', 'gdp'])\n",
    "    for i in range(embeddings.shape[1]):\n",
    "        partial_corr = df[[f'emb_{i}', 'gdp']].corr().iloc[0, 1]\n",
    "        correlations['partial'][f'embedding_{i}'] = (partial_corr, None)\n",
    "    \n",
    "    print(\"\\nCorrelation Analysis:\")\n",
    "    print(f\"Population vs GDP: Pearson r={pearson_pop_gdp:.3f}, p={p_pop_gdp:.3f}\")\n",
    "    print(f\"Population vs GDP: Spearman r={spearman_pop_gdp:.3f}, p={sp_pop_gdp:.3f}\")\n",
    "    print(\"Significant embedding correlations (p < 0.05):\")\n",
    "    for i in range(embeddings.shape[1]):\n",
    "        if correlations['pearson'][f'embedding_{i}'][1] < 0.05:\n",
    "            print(f\"Embedding dim {i}: Pearson r={correlations['pearson'][f'embedding_{i}'][0]:.3f}, p={correlations['pearson'][f'embedding_{i}'][1]:.3f}\")\n",
    "    return correlations\n",
    "\n",
    "def incremental_regression_analysis(embeddings, populations, gdp_values):\n",
    "    \"\"\"Quantify incremental predictive power of embeddings and population\"\"\"\n",
    "    if np.any(~np.isfinite(embeddings)) or np.any(~np.isfinite(populations)) or np.any(~np.isfinite(gdp_values)):\n",
    "        raise ValueError(\"Inputs to incremental regression contain NaN or infinite values\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "        populations_scaled = scaler.fit_transform(populations.reshape(-1, 1))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in scaling for incremental regression: {e}\")\n",
    "        raise\n",
    "    \n",
    "    reg_pop = LinearRegression().fit(populations_scaled, gdp_values)\n",
    "    r2_pop = reg_pop.score(populations_scaled, gdp_values)\n",
    "    \n",
    "    reg_emb = LinearRegression().fit(embeddings_scaled, gdp_values)\n",
    "    r2_emb = reg_emb.score(embeddings_scaled, gdp_values)\n",
    "    \n",
    "    X_full = np.column_stack([embeddings_scaled, populations_scaled])\n",
    "    reg_full = LinearRegression().fit(X_full, gdp_values)\n",
    "    r2_full = reg_full.score(X_full, gdp_values)\n",
    "    \n",
    "    print(\"\\nIncremental Regression Analysis:\")\n",
    "    print(f\"R² (Population only): {r2_pop:.3f}\")\n",
    "    print(f\"R² (Embeddings only): {r2_emb:.3f}\")\n",
    "    print(f\"R² (Full model): {r2_full:.3f}\")\n",
    "    print(f\"Incremental R² from embeddings (beyond population): {r2_full - r2_pop:.3f}\")\n",
    "    print(f\"Incremental R² from population (beyond embeddings): {r2_full - r2_emb:.3f}\")\n",
    "    \n",
    "    return reg_pop, reg_emb, reg_full, r2_pop, r2_emb, r2_full\n",
    "\n",
    "def plot_incremental_r2(r2_pop, r2_emb, r2_full):\n",
    "    \"\"\"Plot bar chart of incremental R² values\"\"\"\n",
    "    labels = ['Population Only', 'Embeddings Only', 'Full Model']\n",
    "    r2_values = [r2_pop, r2_emb, r2_full]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, r2_values)\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Incremental Predictive Power for GDP Prediction')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.text(2, r2_full - 0.05, f'Δ from Pop: +{r2_full - r2_pop:.3f}', ha='center', va='top', color='blue')\n",
    "    plt.text(2, r2_full - 0.10, f'Δ from Emb: +{r2_full - r2_emb:.3f}', ha='center', va='top', color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def extract_urban_metrics(originals):\n",
    "    \"\"\"Extract urban metrics from original images (assuming binary building footprints)\"\"\"\n",
    "    densities = []\n",
    "    avg_sizes = []\n",
    "    num_components = []\n",
    "    \n",
    "    for i, img in enumerate(originals):\n",
    "        if np.any(~np.isfinite(img)):\n",
    "            print(f\"Warning: Image {i} contains NaN or infinite values, skipping\")\n",
    "            densities.append(np.nan)\n",
    "            avg_sizes.append(np.nan)\n",
    "            num_components.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        img_binary = (img > 0.5).astype(np.float64)\n",
    "        density = np.mean(img_binary)\n",
    "        densities.append(density)\n",
    "        \n",
    "        labeled, num = ndimage.label(img_binary)\n",
    "        sizes = ndimage.sum(img_binary, labeled, range(1, num + 1))\n",
    "        avg_size = np.mean(sizes) if num > 0 else 0\n",
    "        avg_sizes.append(avg_size)\n",
    "        num_components.append(num)\n",
    "    \n",
    "    return np.array(densities, dtype=np.float64), np.array(avg_sizes, dtype=np.float64), np.array(num_components, dtype=np.float64)\n",
    "\n",
    "def analyze_urban_metrics(densities, avg_sizes, num_components, gdp_values, populations):\n",
    "    \"\"\"Analyze correlations of urban metrics with GDP and population\"\"\"\n",
    "    metrics = {\n",
    "        'density': densities,\n",
    "        'avg_building_size': avg_sizes,\n",
    "        'num_building_clusters': num_components\n",
    "    }\n",
    "    \n",
    "    print(\"\\nUrban Metrics Analysis:\")\n",
    "    for name, values in metrics.items():\n",
    "        valid_mask = np.isfinite(values) & np.isfinite(gdp_values)\n",
    "        if np.sum(valid_mask) > 1:\n",
    "            pearson_r, p_val = pearsonr(values[valid_mask], gdp_values[valid_mask])\n",
    "            print(f\"{name.capitalize()} vs GDP: Pearson r={pearson_r:.3f}, p={p_val:.3f}\")\n",
    "        else:\n",
    "            print(f\"{name.capitalize()} vs GDP: Insufficient valid data\")\n",
    "        \n",
    "        valid_mask = np.isfinite(values) & np.isfinite(populations)\n",
    "        if np.sum(valid_mask) > 1:\n",
    "            pearson_pop, p_pop = pearsonr(values[valid_mask], populations[valid_mask])\n",
    "            print(f\"{name.capitalize()} vs Population: Pearson r={pearson_pop:.3f}, p={p_pop:.3f}\")\n",
    "        else:\n",
    "            print(f\"{name.capitalize()} vs Population: Insufficient valid data\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_urban_metrics_vs_gdp(densities, avg_sizes, num_components, gdp_values):\n",
    "    \"\"\"Scatter plots of urban metrics vs GDP\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    valid_mask = np.isfinite(densities) & np.isfinite(gdp_values)\n",
    "    axes[0].scatter(densities[valid_mask], gdp_values[valid_mask], alpha=0.5)\n",
    "    axes[0].set_xlabel('Building Density')\n",
    "    axes[0].set_ylabel('GDP per capita ($)')\n",
    "    axes[0].set_title('Building Density vs GDP')\n",
    "    \n",
    "    valid_mask = np.isfinite(avg_sizes) & np.isfinite(gdp_values)\n",
    "    axes[1].scatter(avg_sizes[valid_mask], gdp_values[valid_mask], alpha=0.5)\n",
    "    axes[1].set_xlabel('Average Building Size')\n",
    "    axes[1].set_title('Avg Building Size vs GDP')\n",
    "    \n",
    "    valid_mask = np.isfinite(num_components) & np.isfinite(gdp_values)\n",
    "    axes[2].scatter(num_components[valid_mask], gdp_values[valid_mask], alpha=0.5)\n",
    "    axes[2].set_xlabel('Number of Building Clusters')\n",
    "    axes[2].set_title('Building Clusters vs GDP')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def dimensionality_reduction_visualization(embeddings, gdp_values, populations, cluster_labels):\n",
    "    \"\"\"Perform t-SNE and visualize embeddings\"\"\"\n",
    "    if np.any(~np.isfinite(embeddings)):\n",
    "        raise ValueError(\"Embeddings contain NaN or infinite values for t-SNE\")\n",
    "    \n",
    "    try:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in t-SNE: {e}\")\n",
    "        return None\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    valid_mask = np.isfinite(gdp_values) & np.isfinite(populations)\n",
    "    scatter = plt.scatter(embeddings_2d[valid_mask, 0], embeddings_2d[valid_mask, 1], \n",
    "                          c=gdp_values[valid_mask], cmap='viridis', s=populations[valid_mask]/1e5, alpha=0.7)\n",
    "    plt.colorbar(scatter, label='GDP per capita ($)')\n",
    "    plt.title('t-SNE Visualization of Embeddings (Colored by GDP, Sized by Population)')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeddings_2d[valid_mask, 0], embeddings_2d[valid_mask, 1], \n",
    "                c=cluster_labels[valid_mask], cmap='tab10', s=populations[valid_mask]/1e5, alpha=0.7)\n",
    "    plt.title('t-SNE Visualization of Embeddings (Colored by Clusters, Sized by Population)')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings_2d\n",
    "\n",
    "def outlier_robustness_analysis(embeddings, populations, gdp_values, city_names):\n",
    "    \"\"\"Perform outlier detection and robustness check with RANSAC\"\"\"\n",
    "    if np.any(~np.isfinite(embeddings)) or np.any(~np.isfinite(populations)) or np.any(~np.isfinite(gdp_values)):\n",
    "        raise ValueError(\"Inputs to outlier analysis contain NaN or infinite values\")\n",
    "    \n",
    "    X = np.column_stack([embeddings, populations.reshape(-1, 1)])\n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in scaling for outlier analysis: {e}\")\n",
    "        raise\n",
    "    \n",
    "    reg_ridge = LinearRegression().fit(X_scaled, gdp_values)\n",
    "    residuals_ridge = gdp_values - reg_ridge.predict(X_scaled)\n",
    "    r2_ridge = reg_ridge.score(X_scaled, gdp_values)\n",
    "    \n",
    "    reg_ransac = RANSACRegressor(random_state=42).fit(X_scaled, gdp_values)\n",
    "    inlier_mask = reg_ransac.inlier_mask_\n",
    "    r2_ransac = reg_ransac.score(X_scaled, gdp_values)\n",
    "    \n",
    "    print(\"\\nOutlier Analysis:\")\n",
    "    print(f\"Number of outliers detected: {len(np.where(~inlier_mask)[0])}\")\n",
    "    print(f\"R² (Standard Linear): {r2_ridge:.3f}\")\n",
    "    print(f\"R² (Robust RANSAC): {r2_ransac:.3f}\")\n",
    "    if len(np.where(~inlier_mask)[0]) > 0:\n",
    "        print(\"Top outliers by residual magnitude:\")\n",
    "        outlier_residuals = np.abs(residuals_ridge[~inlier_mask])\n",
    "        sorted_outliers = np.where(~inlier_mask)[0][np.argsort(outlier_residuals)[::-1]][:5]\n",
    "        for idx in sorted_outliers:\n",
    "            print(f\"  {city_names[idx]}: Residual = {residuals_ridge[idx]:.2f}, GDP = {gdp_values[idx]:.2f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(gdp_values, residuals_ridge, c='blue', alpha=0.5, label='All Points')\n",
    "    plt.scatter(gdp_values[~inlier_mask], residuals_ridge[~inlier_mask], c='red', alpha=0.7, label='Outliers')\n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Actual GDP per capita ($)')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Regression Residuals (Red: Outliers)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return reg_ridge, reg_ransac, residuals_ridge, inlier_mask\n",
    "\n",
    "def analyze_clusters_memory_efficient(cluster_labels, city_names, countries, populations, gdp_values, \n",
    "                                     embeddings, originals, reconstructions, n_clusters=7):\n",
    "    \"\"\"Enhanced cluster analysis with statistical tests\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLUSTER ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    correlations = analyze_correlations(embeddings, populations, gdp_values)\n",
    "    reg_pop, reg_emb, reg_full, r2_pop, r2_emb, r2_full = incremental_regression_analysis(embeddings, populations, gdp_values)\n",
    "    \n",
    "    gdp_by_cluster = [gdp_values[cluster_labels == i] for i in range(n_clusters)]\n",
    "    print(\"\\nStatistical Tests for GDP Differences:\")\n",
    "    try:\n",
    "        f_stat, p_value = f_oneway(*[gdp for gdp in gdp_by_cluster if len(gdp) > 0])\n",
    "        print(f\"ANOVA test for GDP differences: F={f_stat:.3f}, p={p_value:.3f}\")\n",
    "    except:\n",
    "        print(\"ANOVA test failed\")\n",
    "    \n",
    "    try:\n",
    "        h_stat, p_value = kruskal(*[gdp for gdp in gdp_by_cluster if len(gdp) > 0])\n",
    "        print(f\"Kruskal-Wallis test: H={h_stat:.3f}, p={p_value:.3f}\")\n",
    "    except:\n",
    "        print(\"Kruskal-Wallis test failed\")\n",
    "    \n",
    "    print(\"\\nPairwise GDP Comparisons:\")\n",
    "    df = pd.DataFrame({'gdp': gdp_values, 'cluster': cluster_labels})\n",
    "    mc = MultiComparison(df['gdp'], df['cluster'])\n",
    "    tukey_result = mc.tukeyhsd()\n",
    "    print(tukey_result)\n",
    "    \n",
    "    print(\"\\nMANOVA for Embeddings Across Clusters:\")\n",
    "    try:\n",
    "        df_manova = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
    "        df_manova['cluster'] = cluster_labels\n",
    "        manova = MANOVA.from_formula(' + '.join([f'emb_{i}' for i in range(embeddings.shape[1])]) + ' ~ cluster', data=df_manova)\n",
    "        print(manova.mv_test())\n",
    "    except Exception as e:\n",
    "        print(f\"MANOVA failed: {e}\")\n",
    "    \n",
    "    print(\"\\nCluster-wise Pearson Correlations (GDP vs Population):\")\n",
    "    cluster_correlations = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        if np.sum(cluster_mask) > 1:\n",
    "            cluster_pop = populations[cluster_mask]\n",
    "            cluster_gdp = gdp_values[cluster_mask]\n",
    "            corr, p_value = pearsonr(cluster_pop, cluster_gdp)\n",
    "            cluster_correlations[cluster_id] = (corr, p_value)\n",
    "            print(f\"Cluster {cluster_id}: r={corr:.3f}, p={p_value:.3f}\")\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_id}: Not enough data for correlation\")\n",
    "            cluster_correlations[cluster_id] = (np.nan, np.nan)\n",
    "    \n",
    "    cluster_stats = {}\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_cities = np.array(city_names)[cluster_mask]\n",
    "        cluster_countries = np.array(countries)[cluster_mask]\n",
    "        cluster_pop = populations[cluster_mask]\n",
    "        cluster_gdp = gdp_values[cluster_mask]\n",
    "        \n",
    "        stats = {\n",
    "            'count': len(cluster_cities),\n",
    "            'gdp_mean': np.mean(cluster_gdp) if len(cluster_gdp) > 0 else np.nan,\n",
    "            'gdp_std': np.std(cluster_gdp) if len(cluster_gdp) > 0 else np.nan,\n",
    "            'gdp_cv': np.std(cluster_gdp) / np.mean(cluster_gdp) if len(cluster_gdp) > 0 and np.mean(cluster_gdp) != 0 else np.nan,\n",
    "            'pop_mean': np.mean(cluster_pop) if len(cluster_pop) > 0 else np.nan,\n",
    "            'pop_std': np.std(cluster_pop) if len(cluster_pop) > 0 else np.nan,\n",
    "            'top_countries': {},\n",
    "            'sample_cities': [],\n",
    "            'all_cities': list(zip(cluster_cities, cluster_countries, cluster_gdp, cluster_pop))\n",
    "        }\n",
    "        \n",
    "        unique_countries, counts = np.unique(cluster_countries, return_counts=True)\n",
    "        for country, count in zip(unique_countries, counts):\n",
    "            stats['top_countries'][country] = count\n",
    "        \n",
    "        gdp_indices = np.argsort(cluster_gdp)[::-1][:5]\n",
    "        for idx in gdp_indices:\n",
    "            city_idx = np.where(cluster_mask)[0][idx]\n",
    "            stats['sample_cities'].append({\n",
    "                'name': cluster_cities[idx],\n",
    "                'country': cluster_countries[idx], \n",
    "                'gdp': cluster_gdp[idx],\n",
    "                'pop': cluster_pop[idx],\n",
    "                'index': city_idx\n",
    "            })\n",
    "        \n",
    "        cluster_stats[cluster_id] = stats\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        print(f\"  Cities: {stats['count']}\")\n",
    "        print(f\"  GDP per capita: ${stats['gdp_mean']:,.0f} ± ${stats['gdp_std']:,.0f} (CV: {stats['gdp_cv']:.3f})\")\n",
    "        print(f\"  Population: {stats['pop_mean']:,.0f} ± {stats['pop_std']:,.0f}\")\n",
    "        top_countries = sorted(stats['top_countries'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        country_str = ', '.join([f'{country}({count})' for country, count in top_countries])\n",
    "        print(f\"  Top countries: {country_str}\")\n",
    "        print(f\"  Sample cities (top 5 by GDP):\")\n",
    "        for city_info in stats['sample_cities']:\n",
    "            print(f\"    • {city_info['name']} ({city_info['country']}): ${city_info['gdp']:,.0f}\")\n",
    "    \n",
    "    return cluster_stats, correlations, reg_full, cluster_correlations, reg_pop, reg_emb, r2_pop, r2_emb, r2_full, gdp_by_cluster\n",
    "\n",
    "def plot_correlation_heatmap(populations, gdp_values):\n",
    "    \"\"\"Plot correlation heatmap between population and GDP\"\"\"\n",
    "    valid_mask = np.isfinite(populations) & np.isfinite(gdp_values)\n",
    "    df = pd.DataFrame({\n",
    "        'Population': populations[valid_mask],\n",
    "        'GDP': gdp_values[valid_mask]\n",
    "    })\n",
    "    \n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.3f',\n",
    "                square=True, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Heatmap: Population vs GDP', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_gdp_distribution_by_cluster(cluster_labels, gdp_values, n_clusters=7):\n",
    "    \"\"\"Plot GDP distribution by cluster with cleaner visualization for large n_clusters\"\"\"\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    gdp_by_cluster = [gdp_values[cluster_labels == i] for i in range(n_clusters)]\n",
    "    \n",
    "    boxplot = plt.boxplot(gdp_by_cluster, patch_artist=True)\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))\n",
    "    for patch, color in zip(boxplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    means = [np.mean(cluster_gdp) for cluster_gdp in gdp_by_cluster]\n",
    "    for i, mean in enumerate(means):\n",
    "        if np.isfinite(mean):\n",
    "            plt.scatter(i+1, mean, color='red', zorder=3, s=30, marker='D')\n",
    "    \n",
    "    plt.xlabel('Cluster', fontsize=12)\n",
    "    plt.ylabel('GDP per capita ($)', fontsize=12)\n",
    "    plt.title(f'GDP Distribution by Cluster (n_clusters={n_clusters})', fontsize=14)\n",
    "    plt.xticks(range(1, n_clusters+1), [f'{i}' for i in range(n_clusters)], rotation=90, fontsize=6)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_population_vs_gdp_by_cluster(cluster_labels, city_names, populations, gdp_values, n_clusters=7):\n",
    "    \"\"\"Plot population vs GDP with cluster coloring and reduced opacity\"\"\"\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        if np.sum(cluster_mask) > 0:\n",
    "            valid_mask = cluster_mask & np.isfinite(populations) & np.isfinite(gdp_values)\n",
    "            plt.scatter(populations[valid_mask], gdp_values[valid_mask], \n",
    "                       c=[colors[cluster_id]], label=f'Cluster {cluster_id}', alpha=0.05, s=100)\n",
    "            \n",
    "            if np.sum(valid_mask) > 1:\n",
    "                slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                    populations[valid_mask], gdp_values[valid_mask])\n",
    "                x_range = np.linspace(np.min(populations[valid_mask]), \n",
    "                                     np.max(populations[valid_mask]), 100)\n",
    "                plt.plot(x_range, slope*x_range + intercept, \n",
    "                        color=colors[cluster_id], linestyle='--', alpha=0.7,\n",
    "                        label=f'Cluster {cluster_id} (r={r_value:.2f})')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Population (log scale)')\n",
    "    plt.ylabel('GDP per capita ($)')\n",
    "    plt.title('Population vs GDP by Cluster with Regression Lines', fontsize=16)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_composition(cluster_stats, n_clusters=7):\n",
    "    \"\"\"Plot histogram of countries in each cluster with smaller x-axis labels\"\"\"\n",
    "    fig, axes = plt.subplots(n_clusters, 1, figsize=(12, 4 * n_clusters))\n",
    "    if n_clusters == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        stats = cluster_stats[cluster_id]\n",
    "        countries = stats['top_countries']\n",
    "        \n",
    "        sorted_countries = sorted(countries.items(), key=lambda x: x[1], reverse=True)\n",
    "        country_names = [c[0] for c in sorted_countries]\n",
    "        country_counts = [c[1] for c in sorted_countries]\n",
    "        \n",
    "        axes[cluster_id].bar(range(len(country_names)), country_counts)\n",
    "        axes[cluster_id].set_title(f'Cluster {cluster_id} - Country Distribution ({stats[\"count\"]} cities)')\n",
    "        axes[cluster_id].set_ylabel('Count')\n",
    "        axes[cluster_id].set_xticks(range(len(country_names)))\n",
    "        axes[cluster_id].set_xticklabels(country_names, rotation=45, ha='right', fontsize=8)\n",
    "        \n",
    "        for i, count in enumerate(country_counts):\n",
    "            axes[cluster_id].text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_images_memory_efficient(cluster_labels, city_names, countries, gdp_values, \n",
    "                                       populations, originals, reconstructions, cluster_id, n_cities=10, select_mode='top_gdp'):\n",
    "    \"\"\"Plot original and reconstructed images for a cluster in a row\"\"\"\n",
    "    cluster_mask = cluster_labels == cluster_id\n",
    "    cluster_indices = np.where(cluster_mask)[0]\n",
    "    if len(cluster_indices) == 0:\n",
    "        print(f\"No cities in cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    if select_mode == 'top_gdp':\n",
    "        cluster_gdp = gdp_values[cluster_mask]\n",
    "        sorted_indices = np.argsort(cluster_gdp)[::-1][:n_cities]\n",
    "        selected_indices = cluster_indices[sorted_indices]\n",
    "        suptitle_text = f'Cluster {cluster_id} - Top {n_cities} Cities by GDP'\n",
    "    elif select_mode == 'one_per_country':\n",
    "        country_counts = Counter(np.array(countries)[cluster_mask])\n",
    "        top_countries = sorted(country_counts, key=country_counts.get, reverse=True)[:min(n_cities, len(country_counts))]\n",
    "        selected_indices = []\n",
    "        for country in top_countries:\n",
    "            country_mask = (cluster_labels == cluster_id) & (np.array(countries) == country)\n",
    "            country_indices = np.where(country_mask)[0]\n",
    "            country_gdps = gdp_values[country_mask]\n",
    "            best_idx = country_indices[np.argmax(country_gdps)]\n",
    "            selected_indices.append(best_idx)\n",
    "        suptitle_text = f'Cluster {cluster_id} - One City per Top {len(top_countries)} Countries (by frequency)'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown select_mode: {select_mode}\")\n",
    "    \n",
    "    n_images = len(selected_indices)\n",
    "    if n_images == 0:\n",
    "        print(f\"No images selected for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    if n_images == 1:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(3, 6))\n",
    "        axes = axes.reshape(2, 1)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, n_images, figsize=(3 * n_images, 6))\n",
    "    \n",
    "    for i, city_idx in enumerate(selected_indices):\n",
    "        city_name = city_names[city_idx]\n",
    "        country = countries[city_idx]\n",
    "        gdp = gdp_values[city_idx] \n",
    "        population = populations[city_idx]\n",
    "        \n",
    "        original_array = originals[city_idx]\n",
    "        if np.any(~np.isfinite(original_array)):\n",
    "            print(f\"Warning: Original image for {city_name} contains invalid values\")\n",
    "            original_array = np.clip(original_array, 0, 1)\n",
    "        \n",
    "        axes[0, i].imshow(original_array, cmap='binary', vmin=0, vmax=1)\n",
    "        axes[0, i].set_title(f\"{city_name}\\nGDP: ${gdp:,.0f}\", fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        reconstruction_array = reconstructions[city_idx]\n",
    "        if np.any(~np.isfinite(reconstruction_array)):\n",
    "            print(f\"Warning: Reconstruction image for {city_name} contains invalid values\")\n",
    "            reconstruction_array = np.clip(reconstruction_array, 0, 1)\n",
    "        \n",
    "        im = axes[1, i].imshow(reconstruction_array, cmap='viridis', vmin=0, vmax=1)\n",
    "        axes[1, i].set_title(f\"Pop: {population:,.0f}\", fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    \n",
    "    cluster_mean_gdp = np.mean(gdp_values[cluster_mask][np.isfinite(gdp_values[cluster_mask])])\n",
    "    cluster_std_gdp = np.std(gdp_values[cluster_mask][np.isfinite(gdp_values[cluster_mask])])\n",
    "    plt.suptitle(f'{suptitle_text}\\nMean GDP: ${cluster_mean_gdp:,.0f} ± ${cluster_std_gdp:,.0f}', \n",
    "                 fontsize=16, y=0.95)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_regression_predictions(embeddings, populations, gdp_values, city_names, cluster_labels, \n",
    "                              reg_pop, reg_emb, reg_full, r2_pop, r2_emb, r2_full, n_clusters=7):\n",
    "    \"\"\"Visualize predictions from linear regression models and embeddings' role in GDP prediction\"\"\"\n",
    "    valid_mask = np.isfinite(populations) & np.isfinite(gdp_values) & np.isfinite(embeddings).all(axis=1)\n",
    "    embeddings = embeddings[valid_mask]\n",
    "    populations = populations[valid_mask]\n",
    "    gdp_values = gdp_values[valid_mask]\n",
    "    city_names = [city_names[i] for i in np.where(valid_mask)[0]]\n",
    "    cluster_labels = cluster_labels[valid_mask]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    populations_scaled = scaler.fit_transform(populations.reshape(-1, 1))\n",
    "    X_full = np.column_stack([embeddings_scaled, populations_scaled])\n",
    "    \n",
    "    pred_pop = reg_pop.predict(populations_scaled)\n",
    "    pred_emb = reg_emb.predict(embeddings_scaled)\n",
    "    pred_full = reg_full.predict(X_full)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].scatter(gdp_values, pred_pop, alpha=0.5, c=cluster_labels, cmap='tab10', s=100)\n",
    "    axes[0].plot([gdp_values.min(), gdp_values.max()], [gdp_values.min(), gdp_values.max()], \n",
    "                 'k--', alpha=0.7, label='Perfect Prediction')\n",
    "    axes[0].set_xlabel('Actual GDP per capita ($)')\n",
    "    axes[0].set_ylabel('Predicted GDP per capita ($)')\n",
    "    axes[0].set_title(f'Population-only Model\\nR² = {r2_pop:.3f}')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].scatter(gdp_values, pred_emb, alpha=0.5, c=cluster_labels, cmap='tab10', s=100)\n",
    "    axes[1].plot([gdp_values.min(), gdp_values.max()], [gdp_values.min(), gdp_values.max()], \n",
    "                 'k--', alpha=0.7, label='Perfect Prediction')\n",
    "    axes[1].set_xlabel('Actual GDP per capita ($)')\n",
    "    axes[1].set_ylabel('Predicted GDP per capita ($)')\n",
    "    axes[1].set_title(f'Embeddings-only Model\\nR² = {r2_emb:.3f}')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].scatter(gdp_values, pred_full, alpha=0.5, c=cluster_labels, cmap='tab10', s=100)\n",
    "    axes[2].plot([gdp_values.min(), gdp_values.max()], [gdp_values.min(), gdp_values.max()], \n",
    "                 'k--', alpha=0.7, label='Perfect Prediction')\n",
    "    axes[2].set_xlabel('Actual GDP per capita ($)')\n",
    "    axes[2].set_ylabel('Predicted GDP per capita ($)')\n",
    "    axes[2].set_title(f'Full Model (Pop + Emb)\\nR² = {r2_full:.3f}')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.suptitle('Actual vs Predicted GDP per Capita by Model', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    try:\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                             c=pred_full, cmap='viridis', s=populations/1e5, alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Predicted GDP per capita ($)')\n",
    "        plt.title('t-SNE of Embeddings (Colored by Full Model Predictions, Sized by Population)')\n",
    "        plt.xlabel('t-SNE Dimension 1')\n",
    "        plt.ylabel('t-SNE Dimension 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in t-SNE visualization: {e}\")\n",
    "\n",
    "def run_memory_efficient_clustering_analysis(embeddings, city_names, countries, originals, reconstructions, \n",
    "                                            n_clusters=7, data_dir=\"city_data\"):\n",
    "    \"\"\"Main function to run clustering analysis with enhanced statistical analysis\"\"\"\n",
    "    print(f\"Starting memory-efficient clustering analysis with {n_clusters} clusters...\")\n",
    "    print(f\"Input data: {len(city_names)} cities\")\n",
    "    \n",
    "    embeddings = validate_inputs(embeddings, city_names, countries, originals, reconstructions, n_clusters)\n",
    "    \n",
    "    (valid_indices, cluster_labels, valid_city_names, valid_countries, \n",
    "     valid_populations, valid_gdp, valid_originals, valid_reconstructions, \n",
    "     features_scaled, kmeans, valid_embeddings) = cluster_cities_memory_efficient(\n",
    "        embeddings, city_names, countries, originals, reconstructions, n_clusters, data_dir\n",
    "    )\n",
    "    \n",
    "    cluster_stats, correlations, reg_full, cluster_correlations, reg_pop, reg_emb, r2_pop, r2_emb, r2_full, gdp_by_cluster = analyze_clusters_memory_efficient(\n",
    "        cluster_labels, valid_city_names, valid_countries, valid_populations, \n",
    "        valid_gdp, valid_embeddings, valid_originals, valid_reconstructions, n_clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Additional Analyses ===\")\n",
    "    \n",
    "    plot_incremental_r2(r2_pop, r2_emb, r2_full)\n",
    "    \n",
    "    densities, avg_sizes, num_components = extract_urban_metrics(valid_originals)\n",
    "    analyze_urban_metrics(densities, avg_sizes, num_components, valid_gdp, valid_populations)\n",
    "    plot_urban_metrics_vs_gdp(densities, avg_sizes, num_components, valid_gdp)\n",
    "    \n",
    "    dimensionality_reduction_visualization(\n",
    "        valid_embeddings, valid_gdp, valid_populations, cluster_labels\n",
    "    )\n",
    "    \n",
    "    outlier_robustness_analysis(\n",
    "        valid_embeddings, valid_populations, valid_gdp, valid_city_names\n",
    "    )\n",
    "    \n",
    "    plot_regression_predictions(\n",
    "        valid_embeddings, valid_populations, valid_gdp, valid_city_names, cluster_labels,\n",
    "        reg_pop, reg_emb, reg_full, r2_pop, r2_emb, r2_full, n_clusters\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    plot_correlation_heatmap(valid_populations, valid_gdp)\n",
    "    plot_gdp_distribution_by_cluster(cluster_labels, valid_gdp, n_clusters)\n",
    "    plot_population_vs_gdp_by_cluster(cluster_labels, valid_city_names, valid_populations, valid_gdp, n_clusters)\n",
    "    plot_cluster_composition(cluster_stats, n_clusters)\n",
    "    \n",
    "    print(\"\\nDisplaying cluster images...\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_size = np.sum(cluster_labels == cluster_id)\n",
    "        if cluster_size > 0:\n",
    "            print(f\"\\nCluster {cluster_id}: {cluster_size} cities\")\n",
    "            plot_cluster_images_memory_efficient(\n",
    "                cluster_labels, valid_city_names, valid_countries, valid_gdp, \n",
    "                valid_populations, valid_originals, valid_reconstructions, \n",
    "                cluster_id, n_cities=min(10, cluster_size), select_mode='top_gdp'\n",
    "            )\n",
    "    \n",
    "    print(\"\\n=== Clusters with Highest and Lowest GDP Variance (min 30 cities) ===\")\n",
    "    large_clusters = [id for id in range(n_clusters) if cluster_stats[id]['count'] >= 30]\n",
    "    if len(large_clusters) < 10:\n",
    "        print(f\"Not enough large clusters (only {len(large_clusters)} found)\")\n",
    "    else:\n",
    "        vars = [(id, cluster_stats[id]['gdp_std']**2) for id in large_clusters if np.isfinite(cluster_stats[id]['gdp_std'])]\n",
    "        vars_sorted = sorted(vars, key=lambda x: x[1])\n",
    "        lowest_5 = vars_sorted[:5]\n",
    "        highest_5 = vars_sorted[-5:][::-1]\n",
    "\n",
    "        print(\"\\nLowest 5 variances:\")\n",
    "        for id, var in lowest_5:\n",
    "            print(f\"Cluster {id}: variance={var:,.2f}, count={cluster_stats[id]['count']}, mean_gdp=${cluster_stats[id]['gdp_mean']:,.2f}\")\n",
    "            country_counts = sorted(cluster_stats[id]['top_countries'].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Country counts:\", country_counts)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            country_names = [c[0] for c in country_counts]\n",
    "            counts = [c[1] for c in country_counts]\n",
    "            ax.bar(range(len(country_names)), counts)\n",
    "            ax.set_title(f\"Cluster {id} - Country Distribution ({cluster_stats[id]['count']} cities)\")\n",
    "            ax.set_ylabel('Number of Cities')\n",
    "            ax.set_xticks(range(len(country_names)))\n",
    "            ax.set_xticklabels(country_names, rotation=45, ha='right', fontsize=8)\n",
    "            for i, count in enumerate(counts):\n",
    "                ax.text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            gdps = np.array([city[2] for city in cluster_stats[id]['all_cities'] if np.isfinite(city[2])])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.boxplot(gdps, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "            plt.title(f\"GDP Boxplot for Cluster {id} (Lowest Variance)\")\n",
    "            plt.ylabel('GDP per capita ($)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            plot_cluster_images_memory_efficient(\n",
    "                cluster_labels, valid_city_names, valid_countries, valid_gdp, \n",
    "                valid_populations, valid_originals, valid_reconstructions, \n",
    "                id, n_cities=10, select_mode='one_per_country'\n",
    "            )\n",
    "\n",
    "        print(\"\\nHighest 5 variances:\")\n",
    "        for id, var in highest_5:\n",
    "            print(f\"Cluster {id}: variance={var:,.2f}, count={cluster_stats[id]['count']}, mean_gdp=${cluster_stats[id]['gdp_mean']:,.2f}\")\n",
    "            country_counts = sorted(cluster_stats[id]['top_countries'].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Country counts:\", country_counts)\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            country_names = [c[0] for c in country_counts]\n",
    "            counts = [c[1] for c in country_counts]\n",
    "            ax.bar(range(len(country_names)), counts)\n",
    "            ax.set_title(f\"Cluster {id} - Country Distribution ({cluster_stats[id]['count']} cities)\")\n",
    "            ax.set_ylabel('Number of Cities')\n",
    "            ax.set_xticks(range(len(country_names)))\n",
    "            ax.set_xticklabels(country_names, rotation=45, ha='right', fontsize=8)\n",
    "            for i, count in enumerate(counts):\n",
    "                ax.text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            gdps = np.array([city[2] for city in cluster_stats[id]['all_cities'] if np.isfinite(city[2])])\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.boxplot(gdps, patch_artist=True, boxprops=dict(facecolor='lightcoral'))\n",
    "            plt.title(f\"GDP Boxplot for Cluster {id} (Highest Variance)\")\n",
    "            plt.ylabel('GDP per capita ($)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            plot_cluster_images_memory_efficient(\n",
    "                cluster_labels, valid_city_names, valid_countries, valid_gdp, \n",
    "                valid_populations, valid_originals, valid_reconstructions, \n",
    "                id, n_cities=10, select_mode='one_per_country'\n",
    "            )\n",
    "    \n",
    "    return {\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'valid_indices': valid_indices,\n",
    "        'cluster_stats': cluster_stats,\n",
    "        'silhouette_score': silhouette_score(features_scaled, cluster_labels),\n",
    "        'valid_city_names': valid_city_names,\n",
    "        'valid_countries': valid_countries,\n",
    "        'valid_populations': valid_populations,\n",
    "        'valid_gdp': valid_gdp,\n",
    "        'correlations': correlations,\n",
    "        'cluster_correlations': cluster_correlations,\n",
    "        'regression_model': reg_full,\n",
    "        'kmeans_model': kmeans\n",
    "    }\n",
    "\n",
    "try:\n",
    "    results = run_memory_efficient_clustering_analysis(\n",
    "        embeddings=embeddings,\n",
    "        city_names=city_names,\n",
    "        countries=countries,\n",
    "        originals=originals,\n",
    "        reconstructions=reconstructions,\n",
    "        n_clusters=100, \n",
    "        data_dir=\"city_data\"\n",
    "    )\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}. Please ensure all input variables are defined.\")\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}. Check for invalid values in embeddings, populations, GDP, or building density data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cultural_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
